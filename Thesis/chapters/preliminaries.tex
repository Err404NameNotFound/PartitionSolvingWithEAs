%% preliminaries.tex
%%

%% ==============
\chapter{Preliminaries}\label{ch:preliminaries}
%% ==============

%This chapter should provide the foundations of the thesis.
\section{Notation}
A short list of vocabulary used throughout the paper.
\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries RSH}]
    \item[EA] Evolutionary Algorithm
    \item[RSH] Randomised Search Heuristic referring to all analysed Evolutionary algorithms
    \item[$n$] The input length of the problem
    \item[$w_i$] The $i$-th object of the input. If not mentioned otherwise the weights are sorted in non-increasing order so: \(w_1 \ge w_2 \ge \ldots \ge w_{n-1} \ge w_{n}\)
    \item[$W$] The sum of all objects: W = $\sum_{i=1}^{n}w_i$
    \item[bin] When solving Partition a set of numbers is divided into two distinct subsets and in this paper both subsets are referred to as bins
    \item[$b_F$] The fuller bin (the bin with more total weight)
    \item[$b_E$] The emptier bin (the bin with less total weight)
%     \item[$b_{w_i}$] The bin containing the object $w_i$
    \item[$opt$] The optimal solution for a given partition instance.
    \item[$x$] A vector $x \in {\{0, 1\}}^n$ describing a solution
    \item[$f(x)$] The fitness function for PARTITION.\ This means a solution $x$ has a solution quality of \(f(x)=\max\{\sum_{i=1}^{n}w_i\cdot x_i, \sum_{i=1}^{n}w_i\cdot(1-x_i)\}=b_F\)
\end{description}

\section{Background}
\subsection{Known algorithms for partition}
Multiple methods for generating a solution of PARTITION already exist.
Solving the problem with a greedy approach in runtime $\mathcal{O}(n)$ results in an approximation ratio of 3/2 if the elements are not sorted or a ratio of 7/6 if the numbers are sorted~\cite{graham1966bounds}.
Greedy in this case means putting each element in the currently emptier set while looking at each value exactly once.
Another approximation algorithm is the KK-algorithm or also called Largest Differencing Method.
With expected time $\mathcal{O}(n\log{}n)$ it has the same runtime as greedy with sorting and also the same worst case approximation of 7/6.
For inputs chosen uniform random from [0,1] the KK has an expected ratio of \(1+\frac{1}{n^{\Theta(\log{}n)}}\) in comparison to the greedy algorithm which only reaches an approximation ratio of \(1+\mathcal{O}(\frac{1}{n})\).
Instead of putting each element in the currently emptier set the currently largest two values are combined to one value by either subtracting or adding them depending on which results in a better solution.
Adding them corresponds to putting the elements in the same set whereas subtracting them means putting them in different sets. 
There is even a fully polynomial time approximation algorithm (FPTAS) for the subsetsum problem~\cite{KELLERER2003349} which can be used for PARTITION by setting the required sum to $\lfloor W/2\rfloor$.
FPTAS return a solution of at most (1+$\epsilon$) the optimum in a time that is polynomial both in $n$ and in $\frac{1}{\epsilon}$.
There are lots of other approximation-algorithms but also some algorithms that always return the best solution.
The Pseudopolynomial time number partitioning algorithm which uses dynamic programming always returns an optimal solution but needs time and space $\mathcal{O}(n\frac{m}{2})$ where $m$ is the largest number in the input~\cite{korf2009multi}.
The runtime is only pseudopolynomial because to encode $m$ in the input only $\log_{2}{(m)}$ bits are required which causes \(m=2^{\log_{2}{(m)}}\) to be exponential in the input size.
The Complete Greedy Algorithm (CGA)~\cite{korf1998complete} traverses a binary tree depth first and searches the complete $2^n$ search space in a greedy way.
It functions the same way as the simple greedy algorithm but instead of only looking at only the greedy option at each height it also evaluates the non-greedy option after evaluating the complete subtree of the greedy option.
The algorithm continues the depth first search until it either finds a perfect partition or has traversed the whole tree.
In the second case it will return the best value found on the way.
While the space complexity in only $\mathcal{O}(n)$ the runtime is $\mathcal{O}(2^n)$.
Another exact algorithm is the Complete Karmarkar-Karp (CKK)~\cite{korf1998complete}.
This algorithm works similar to the complete greedy approach by traversing the binary tree of all solutions.
Instead of greedily selecting the next edge here the algorithm behaves like the KK-algorithm described above.
It performs better than the CGA for the same reasons as before but also has the same worst case running time as the GCA.

\subsection{Evolutionary Algorithm}
Evolutionary Algorithms mimic the process of evolution and normally behave mostly the same.
A run typically looks like this:
\begin{enumerate}
      \item Generate initial population at random
      \item If stopping condition are met return the currently best solution
      \item Generate offspring population (e.g.\ by mutation)
      \item Evaluate fitness of the offspring
      \item Select fittest individuals and update population
      \item Go back to step 2.
\end{enumerate}
For PARTITION a solution $x\in{\{0,1\}}^{n}$ separates all numbers into two different sets with $x_i=0$ meaning $w_i$ is in set 0 whereas $x_i=1$ meaning $w_i$ is in set 1.
So every possible value of $x$ describes a feasible solution but not necessarily a good one.
To evaluate the quality of a solution the EA is given a fitness function.
The fitness function in this case is \(f(x)=\max\{\sum_{i=1}^{n}w_i\cdot x_i, \sum_{i=1}^{n}w_i\cdot(1-x_i)\}=b_F\).
The goal of the algorithm is to return a solution with minimal fitness.
A mutation step in the PARTITION problem will change an algorithm-dependent number of bits from 1 to 0 or vice versa.
In this case flipping a bit means putting the element in the other set.
A simple implementation of an EA is the so called (1+1) EA (Algorithm~\ref{alg:EA}).\
The first 1 in the brackets refers to size of the population and the second to the amount of mutants created in each iteration of the loop.
So it always has only one solution and generates just one new solution in each step.
The mutation of the current individual is performed by flipping each bit independently with probability $1/n$.
The amount of flipped bits is binomial distributed with an expected value of $n\cdot\frac{1}{n}=1$.
% By changing the mutation rate $1/n$ to $c/n$ for any constant $c$ the algorithm now flips $n\cdot c/n=c$ bits in expectation.\newline
\begin{algorithm}[bt]
      \caption{\textsc{(1+1) EA}}\label{alg:EA}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow x$\;
      flip every bit of $x'$ with probability $1/n$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}
Another simple EA is the Randomised Local Search (RLS see Algorithm~\ref{alg:RLS}).\
Instead of flipping each bit with probability $1/n$ here one bit is flipped at uniform random.
Apart from that the algorithms are the same.
\begin{algorithm}[bt]
      \caption{\textsc{RLS}}\label{alg:RLS}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow x$\;
      flip one uniform random bit of $x'$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

\subsection{Literature on the RLS and (1+1) EA for PARTITION}
Carsten Witt proved that the RLS and the (1+1) EA find a $(4/3+\epsilon)$ approximation in expected time $\bigO(n)$ and a $(4/3)$-approximation in expected time $\bigO(n^2)$~\cite{witt2005worst}.
He then introduced an almost worst case input to prove the bound for the approximation ratio is at least almost tight.
The input is defined as followed for any $0<\epsilon<1/3$ and even $n$:\newline
The input contains two numbers of value $1/3 - \epsilon/4$ and $n-2$ elements of value $(1/3+\epsilon/2)/(n-2)$. 
The total volume is normalised to 1.
When the two large values are in the same bin, the RSHs are tricked into a local optimum, where only $w_1$ and $w_2$ are in the first bin and the remaining elements in the other bin.
This results in an almost worst case.
To leave this local optimum $\Omega(n)$ bits must be moved in a step separating the two large values.
Such a step will never happen for the RLS and only in expected exponential time for the (1+1) EA.\
This worst case happens with probability $\Omega(1)$.
He also proved both RSHs return a (1+$\epsilon$)-approximation for $\epsilon\ge4n$ in expected time \(\lceil en\ln(4/\epsilon)\rceil\) for the (1+1) EA and \(\lceil en\ln(4/\epsilon)\rceil\) for the RLS with probability at least \(2^{-(e\log{e}+e)\lceil 2/\epsilon\rceil \ln(4/\epsilon)-\lceil 2/\epsilon\rceil}\) for the (1+1) EA and at least \(2^{-(\log{e}+1)\lceil 2/\epsilon\rceil \ln(4/\epsilon)-\lceil 2/\epsilon\rceil}\) for the RLS.\
Afterwards he proved both RHSs reach a solution where the difference between the two bins is at most 1 for uniform distributed inputs on [0,1] after expected time $\bigO(n^2)$ for the (1+1) EA and $\bigO(n\log{n})$ for the RLS.\
The difference between the two bis is even bounded by $\bigO(\log{n}/n)$ after $\bigO(n^{c+4}\log{n})$ steps with probability at least $1-\bigO(1-1/n^c)$.
This leads to an expected difference of $\bigO(\log{n}/n)$ after $\bigO(n^{c+4}\log{n})$ steps.
He also analysed exponential distributed inputs with parameter 1.
With probability $1-\bigO(1/n^c)$ the difference on those inputs is bounded by $\bigO(\log{n})$ after $\bigO(n^2\log{n})$ steps and even by $\bigO(\log{n}/n)$ after $\bigO(n^{c+4}\log^2{n})$ steps.
Additionally he described a polynomial time randomised approximation scheme (PRAS) for the RLS and the (1+1) EA for values of $\epsilon=\Omega(\log{\log{}}n/\log{n})$.\newline
For MAKESPAN-SCHEDULING a list of processing times has to be distributed on a set of machines while minimising the total time of the fullest machine.
With 2 machines this problem is exactly the same as PARTITION.\
So in a sense MAKESPAN-SCHEDULING is a more general version of PARTITION.\
This lead to Christian Gunia generalising some results previously shown by C. Witt to MAKESPAN-SCHEDULING on $k$ machines~\cite{gunia2005analysis}.
Solutions for MAKESPAN-SCHEDULING are \(x\in{\{0,\dots,k-1\}}^n\) and therefore during a mutation $x_i$ is set to a uniform random value from $\{0,\dots,k-1\}\text{\textbackslash}\{x_i\}$ instead of $1-x_i$.
The adapted RSHs reach an approximation ratio of $(2k/k+1)$ in expected time $\bigO(Wn^{2k-2}/w_n)$.
On an instance where every weight is the same the expected optimisation time is bounded by $\bigO(n\log{n})$.
He also adapted the almost worse case to the more general problem and proved the RLS does not find a solution better than \((2k/(k+1)-\epsilon)\) in finite time for any $\epsilon>0$ with constant probability.
The second statement for PARTITION on the uniform distributed inputs on [0,1] has an equivalent lemma for MAKESPAN-SCHEDULING on $k$ machines as well.

Another way of dealing with $NP$-hard problems is identifying a parameter $k$ which defines how hard the problem is to solve.
One possible parametrisation of PARTITION is solving whether there is a solution of $f(x)\le k$.
A fixed-parameter tractable problem is a problem that can be solved in time polynomial in the size of the input and $g(k)$ where $g$ is any arbitrary function.
Partition falls into this category as it can be decided in time at most $\bigO(4^k)$\cite{fernau2005parameterized}.
Andrew M. Sutton and Frank Neumann made a parametrised analysis of PARTITION\cite{sutton2012parameterized}.\
They parametrised the problem in multiple ways.
One of their parametrisation was: given an integer $k$, is there a solution of at most $W/2+W/k$?
They showed that a multistart of the (1+1) EA or RLS using runs of length \(\lceil en\ln(2k)\rceil\) is a Monte Carlo-fpt algorithm for this parametrised version of PARTITION.\
They also analysed a parametrisation of the size of the critical path and also the discrepancy (the difference between the two bins).

\subsection{Problems for Evolutionary Algorithms}
There are two well analysed problems for Evolutionary Algorithms which are relevant for this thesis.
One of those is OneMax.
For OneMax the fitness function $f(x)=\sum_{i=1}^{n}{x_i}$ has to be maximised.
In the optimal solution every bit is set to one.
This problem is rather easy as it has only one global optimum and no local optimum.
So every step the algorithms makes decreases the Hamming distance to the optimum if the fitness increases.
A more general version of this problem are linear functions where every bit is given a weight.
Here the fitness function is $f(x)=\sum_{i=1}^{n}{w_i \cdot x_i}$ which either has to be maximised or minimised.
The weights $w_i$ can be any real value.
In contrast to OneMax increasing the fitness can increase the Hamming distance to the global optimum if multiple small values switch places with a big value.
The runtime for both the RLS and the (1+1) EA is $\Theta(n\log{n})$ for both problems and the optimal mutation rate for the (1+1) EA is $1/n$ which was proven multiple times (\cite{witt2013tight},~\cite{doerr2023tight}).
The optimality of $1/n$ does not hold for every problem which leads to the next section.

\section{Higher mutation rates and heavy tailed mutations}
The RLS and the standard (1+1) EA flip one bit in expectation which is optimal for some problems as seen in the last section.
This is not the case for every fitness function.
$\text{Jump}_k$ has an optimal mutation rate of $k/n$ and a small constant factor deviation from $k/n$ results in an increase of the runtime exponential in $\Omega(k)$\cite{doerr2017fast}.
The same might hold for PARTITION, because the previously discussed literature only analyses mutation rates with a 1-bit flip in expectation.\newline
One way of creating algorithms with higher mutation rates is adjusting the currently existing algorithms.
For the (1+1) EA this can be done easily.
By changing the mutation rate $1/n$ to $c/n$ for any constant $c$ the algorithm now flips $n\cdot c/n=c$ bits in expectation.
\begin{algorithm}[bt]
      \caption{\textsc{(1+1) EA with static mutation rate}}\label{alg:EA_SM}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow x$\;
      flip every bit of $x'$ with probability $c/n$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

For the RLS it is not that simple, as the RLS chooses a random bit and flips it.
Instead of flipping c bits in every step there should be the possibility to flip different amounts of bits in every step.
The standard RLS chooses a random neighbour with Hamming distance one.
So the variant of the RLS could simply choose neighbours that have a Hamming distance larger than one.
The selection should still be uniform random to keep the idea of the RLS intact.
One possible way is to choose a random neighbour with Hamming distance $\le k$.
This algorithm will be called \RLSN[k] from now on, because it chooses a random neighbour within the Hamming ball with radius $k$.
The amount of neighbours with Hamming distance $y$ is given by $\binom{n}{y}$.
For $k=4$, this results in $n$ neighbours with Hamming distance 1, $n(n-1)/2$ neighbours with Hamming distance 2, $n(n-1)(n-2)/6$ for 3
and $n(n-1)(n-2)(n-3)/24$ for 4.
The probability to choose a random neighbour with Hamming distance $y \le k$ for $k = \mathcal{O}(1)$ is given by
\[P(\text{\RLSN}\text{ flips }y\text{ bits}) = \frac{\binom{n}{y}}{\sum_{i=1}^k \binom{n}{i}} = \frac{\Theta(n^y)}{\sum_{i=1}^k \Theta(n^i)}
      = \frac{\Theta(n^y)}{\Theta(n^k)} = \Theta(n^{y-k}) = \Theta(\frac{1}{n^{k-y}})\]
This variant of the RLS is likely to choose a neighbour with Hamming distance k as the number of neighbours with hamming
distance $k$ rises with $k$ for $k \le n/2$.
The probability of flipping only one bit is $\mathcal{O}(\frac{1}{n^{k-1}})$.
For some inputs flipping only one bit might be more optimal which is rather unlikely for this variant of the RLS.\newline
An alternative way of changing the RLS is to first choose $y \in \{1, \dots, k\}$ uniform random and then choose a neighbour with Hamming distance $y$ uniform random.
Here the probability of flipping $y \le k$ bits is given by $1/k$, so the algorithm is much more likely to choose to flip only one bit.
This variant of the RLS will be referred to as \RLSR[k] because it first choses the Hamming sphere and afterwards the neighbour within the selected Hamming sphere.

\begin{algorithm}[bt]
      \caption{\textsc{\RLSN}}\label{alg:rlsN}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow \text{uniform random neighbour of x with Hamming distance} \le k$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

\begin{algorithm}[bt]
      \caption{\textsc{\RLSR}}\label{alg:rlsR}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $y \leftarrow \text{uniform random value }\in \{1,\dots,k\}$\;
      $x' \leftarrow \text{uniform random neighbour of x with Hamming Distance } y$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

Both variants of the RLS change at most $k$ bits in each step and therefore only a constant amount of bits.
For the (1+1) EA the algorithm will also flip mostly $\mathcal{O}(c)$ bits which is also constant.
So neither of the new variants is likely to change up to $\Theta(n)$ bits.
Quinzan \textit{et al.} therefore introduced another mutation operator in~\cite{friedrich2018evolutionary} called $pmut_\beta$.
This operator chooses $k$ from a powerlaw distribution $D^\beta_n$ with exponent $\beta$ and maximum value $n$ and then flips $k$ uniform random bits.
This algorithm will mostly flip a small number of bits but occasionally up to n bits.
Mutation operators like this are called heavy tailed mutations because their tail is not bounded exponentially.