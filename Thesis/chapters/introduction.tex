%% introduction.tex
%%

%% ==============================
\chapter{Introduction}\label{ch:introduction}
%% ==============================

% This chapter should contain
% \begin{enumerate}
%   \item A short description of the thesis topic and its background.
%   \item An overview of related work in this field.
%   \item Contributions of the thesis.
%   \item Outline of the thesis.
% \end{enumerate}

The question of $P=NP$ is still unanswered to this day and solving $NP$-hard problems for every instance still requires exponential time.
To avoid the exponential running time on $NP$-hard optimisation problems one might use approximation algorithms.
Those algorithms do not always return the best possible solution but only a solution with a guaranteed solution quality.
For a minimisation problem a (1+0.5)-approximation algorithm will always return a solution that has at most 1.5 times the optimal value.
An example of an $NP$-hard optimisation problem is PARTITION.\ 
An instance of this problem is a multiset of $n$ positive numbers $\{w_1,\dots,w_n\}$ which has to be divided in two subsets with sums that are as close as possible.
So a solution of PARTITION is a subset of $I\subset \{1,\dots,n\}$ which splits the multiset into two subsets.
The quality of the solution then is $\max\{\sum_{i\in I}w_i, \sum_{i\notin I}w_i\}$.
PARTITION is one of the easiest $NP$-hard problems and has even been dubbed the easiest $NP$-hard problem~\cite{hayes2002computing}.
There are multiple algorithms specifically designed for PARTITION.\ 
Some of them return approximations and others always return the best solution.
The exact algorithms have a runtime exponential in the input size due to the $NP$-hardness.
Problem specific approximation algorithms are mostly deterministic such as the greedy method, the KK-algorithm or the FPTAS for the subsetsum problem which can be used for partition as well.
Another class of non-deterministic algorithms are the so-called Evolutionary Algorithms which mimic the behaviour of evolution.
Those algorithms start with a random population of solutions which are then changed with random steps.
If the solution is good enough it survives and replaces one of the worst individuals in the population.
The EA continues to generate new offspring in an endless loop.
In practice the algorithm is given stopping conditions such as reaching a number of iterations or a specific solution quality.
This is the principle of an anytime algorithm which can be terminated at anytime and output a valid solution.
The longer the waiting time the better the solution might get.
The main usage of EA lies in problems without a problem specific algorithms as these mostly outperform the EAs.
To better understand their behaviour analysing them on well researched problems might still be beneficial to learn more about this class of algorithms.
This thesis researches the runtime of basic EAs such as (1+1) EA and variations of the RLS.\ 
The first part is a theoretical analysis with a main focus of lowering bounds that were previously shown.
Additionally there are new results for other algorithm variants and also a lemma on different type of inputs.
The remainder of this thesis is an empirical analysis of multiple base algorithms with different parameter setting on different kinds of inputs.
Here not only the (1+1) EA with different mutation rate and variants of the RLS with different parameter values are researched but also a heavy tailed mutation operator.
Apart from typical distributions such as the uniform, geometric and binomial distribution there are also results for problem specific instances such as an input where one values is as large as all other values combined.
In the end the empirical results are condensed into a personal suggestion which algorithm should best be chosen to solve the problem, depending on the input but also in general.

