\chapter{Experimental Results}\label{ch:expRes}

In the following chapter the different variants of the RLS and the (1+1) EA are now analysed empirically for the best algorithm depending on the input.
Additionally for most lemmas from the previous chapters there are also tests if they actually hold in practice.

\section{Code}
The complete java code used for all empirical studies is available on \href{https://github.com/Err404NameNotFound/PartitionSolvingWithEAs}{GitHub}.
\subsection{The Algorithms}
All different variants of the RLS function more or the less the same. They start with an initial random value and then optimise this one value in the loop. The loop can be summarised like this:
\begin{enumerate}
      \item generate a number $k$ of bits to be flipped (algorithm specific)
      \item flip $k$ random bits
      \item evaluate fitness of the mutated individual
      \item replace old value with new value if new value is better
      \item repeat if not optimal
\end{enumerate}
The (1+1) EA variants behave differently at first glance as they flip each bit independently with probability $c/n$.
This can be seen as $n$ independent Bernoulli trials with probability $c/n$.
The amount of bits that are flipped is therefore binomial distributed and the algorithm can be implemented exactly as the versions of the RLS. The same holds for the $pmut$ operator which generates a number $k$ from a powerlaw distribution and then flips $k$ bits.
This leads to only one implementation of a partition solving algorithm which is not only given the input array of numbers but also a generator for the amount of bits to be flipped in each step.
The random values for the amount of bits to be flipped are generated according to this table:

\begin{tabular}[h]{c c}
      Algorithm & Returned value                                                                          \\
      \hline
      RLS       & 1                                                                                       \\
      \RLSN~    & $y \in \{1,\dots,k\}$ with probability $\frac{\binom{n}{y}}{\sum_{i=1}^k \binom{n}{i}}$ \\
      \RLSR~    & uniform random value $y \in \{1,\dots,k\}$                                              \\
      (1+1) EA  & binomial distributed value from \textasciitilde$B(n,c/n)$                               \\
      pmut      & powerlaw distributed value from \textasciitilde$D^\beta_{n/2}$                          \\
\end{tabular}



\begin{algorithm}[bt]
      \caption{\textsc{GenericPartitionSolver}}\label{alg:genericPartition}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform random from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow x$\;
      $k \leftarrow \text{kGenerator.generate()}$\;
      flip $k$ uniform random bits of $x'$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

\subsection{Random number generation}
Java only provides a random number generator for uniform distributed values for any integer interval or random double values $\in \left[0, 1\right)$.
The same holds for the \href{http://www.math.sci.hiroshima-u.ac.jp/m-mat/MT/emt.html}{MersenneTwister} with an implementation used from this \href{https://cs.gmu.edu/~sean/research/}{page}.
All experiment were executed with both uniform random number generators.
The results were rather similar, so only the results for the Fast Mersenne Twister are shown.
For this project uniform random numbers do not suffice as for an efficient way of implementing the (1+1) EA or simply for generating a binomial distributed input another random number generator is needed.
One of the needed distributions is a binomial distribution.
The simplest way to generate a number \textasciitilde$B(m,p)$ would be to run a loop $m$ times and add 1 to the generated number if a uniform random value $\in \left[0, 1\right)$ is less than $p$.
This works perfectly fine and generates numbers according to the distribution.
With low values for p this approach is rather inefficient and especially for values of $p=1/m$.
The expected value in this case is 1 but generating a random number takes time $\mathcal{O}(m)$.
Another more efficient way was implemented by StackOverflow user \href{https://stackoverflow.com/users/2166798/pjs}{pjs} on \href{https://stackoverflow.com/questions/23561551/a-efficient-binomial-random-number-generator-code-in-java}{stackoverflow} inspired by Devroyes method introduced in~\cite{devroye2006nonuniform}.
This method has an expected running time of $\mathcal{O}(mp)$ which is equal to the expected value of the distribution.
For the case of $p=1/m$ this runs in expected constant time in comparison to $\mathcal{O}(m)$ for the naive way.
This number generation was also used for the implementation of the (1+1) EA instead of running a loop in every step.
Algorithm~\ref{alg:binomialRNG} uses the second waiting time method which uses the fact that X\textasciitilde$B(m,p)$ if X is the smallest integer so that \(\sum_{i=1}^{X+1}{\frac{E_i}{n-i+1}}<-\log(1-p)\) for $E_i$ iid exponential random variables (Lemma 4.5 section X.4~\cite{devroye2006nonuniform}).

\begin{algorithm}[h]
      \caption{\textsc{Binomial random number generator}}\label{alg:binomialRNG}

      % Some settings
      \DontPrintSemicolon%dontprintsemicolon
      \SetFuncSty{textsc}
      $q \leftarrow \ln(1.0 - p)$\;
      $x \leftarrow 0$\;
      $sum \leftarrow 0$\;
      \While{true}
      {
      $sum \leftarrow sum +\ln(\text{random()}) / (n - x)$\; \tcp{random() generates a random value $\in \left[0, 1\right)$}
      \If{sum < q}
      {
            return $x$\;
      }
      $x \leftarrow x + 1$\;
      }
\end{algorithm}

The next generator needed is for geometric distributed values.
This generator is only necessary for the generation of geometric distributed inputs but not for the algorithms.
The easiest way to generate geometric distributed values is the naive way:
generating a uniform random value $p'$ until $p'<p$ holds.
The expected running time of this algorithm is equal to the expected value of the distribution $1/p$.
So this method is comparably effective to the approach used for binomial random number generation.

\begin{algorithm}[h]
      \caption{\textsc{Geometric random number generator}}\label{alg:geometricRNG}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}
      $sum \leftarrow 0$\; \tcp{random() generates a random value $\in \left[0, 1\right)$}
      \While{\text{random()} $\ge$ q}
      {
            $sum \leftarrow sum+1$\;
      }
      return $sum$\;
\end{algorithm}

The last generator needed is for powerlaw distributed values.
This generator is in contrast to the geometric number generator needed for both the algorithm with the $pmut_\beta$ mutation operator and for generating inputs.
This implementation is also from stackoverflow. The user \href{https://stackoverflow.com/users/52738/gnovice}{gnovice} provided the following formula on \href{https://stackoverflow.com/questions/918736/random-number-generator-that-produces-a-power-law-distribution}{this} page on stackoverflow:
\[
      x = {[(b^{n+1} - a^{n+1})*y + a^{n+1}]}^{1/(n+1)}
\]
$a$ is the lower bound, $b$ the upper bound, $n$ the parameter of the distribution and $y$ the number generated uniform random $\in \left[0, 1\right)$.
The idea behind the formula and the formula itself is explained in a \href{https://mathworld.wolfram.com/RandomNumber.html}{mathworld} page.
For a powerlaw distribution \(P(x)=Cx^n\) for \(x\in[a,b]\) normalisation gives
\[\int_{a}^{b}{P(x)dx}=C\frac{{[x^{n+1}]}^{b}_{a}}{n+1}=1\Leftrightarrow C = \frac{n+1}{b^{n+1}-a^{n+1}}\]
Let Y be a uniformly random distributed variate on [0,1]. Then
\[D(x)=\int_{a}^{x}{P(x')dx'}=C\int_{a}^{x}{{x'}^{n}dx'}=\frac{C}{n+1}(x^{n+1}-a^{n+1})=\frac{(x^{n+1}-a^{n+1})}{b^{n+1}-a^{n+1}}\equiv y\]
and the variate is given by
\[X={[(b^{n+1} - a^{n+1})*y + a^{n+1}]}^{1/(n+1)}\]
The values inserted in this formula must be negative.
In the original paper for the $pmut_{\beta}$ operator and in the definition normally a powerlaw distribution is \(P(x)=Cx^{-n}\) and therefore any positive value for $n$ in this case was negated.
Apart from this negation the generator was not changed.

\section{Do binomial inputs have perfect partitions?}

Lemma~\ref{lemma:BinomialSolvable} is only valid for larger $n$.
In practice the bound is much smaller depending on the expected value of a single number.
Another factor deciding how likely an input is to have a perfect partition is whether $n$ is even or odd.
To determine the influence of all factors multiple experiments were conducted.
The goal of the first experiment was to determine the influence of the array size to the input having a perfect partition and the fact if $n$ is even or odd.
So for every possible combination of $p \in \{0.1, 0.2, \dots , 0.8, 0.9\}, m \in \{10,100,1000,10^4,10^5\} \text{ and } n \in \{2,3,4,\dots,19,20\}$ 1000 randomly generated inputs of size $n$ were tested for a perfect partition.
Due to the small values for $n$ it was possible to brute force the results in a short amount of time.
The results are visualised in figure~\ref{fig:firstBinPercentage} to figure~\ref{fig:lastBinPercentage}.

\begin{figure}[h]
      \caption{Percentage of Binomial inputs with perfect partitions for $m = 10$}
      \centering
      \includegraphics[width=0.45\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m10.png}\label{fig:firstBinPercentage}
\end{figure}

On the $x$-axis is the size of the input and on the $y$-axis the percentage of inputs that had a perfect partition.
The different graphs in each figure resemble the different values of $p$ used for generating the inputs.
The graph for $p=0.1$ resembles the percentage of inputs that had a perfect partition with values generated from the distribution \textasciitilde$B(m,0.1)$ with $m$ being dependent on the figure.
For figure~\ref{fig:firstBinPercentage} $m$ has the value 10.\newline
Figure~\ref{fig:firstBinPercentage} is a bit overloaded with information and the zigzag makes it hard to gain any benefit from the graphs.
That's why for $m \in \{10,100,1000\}$ there is one figure for the even input sizes of $n$ and one for the odd.
Figures which show only results for either even or odd values of $n$ have dotted graphs, because the values in between the points do not exist.
The dotted lines are only in the figure for a better visualisation of the trend and not meant for interpretation apart from the marked values.
For $n\ge10,000$ all values for the odd input sizes are 0 \%, so there is no point in showing the data in a separate figure.

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 10$ for even $n$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m10_even.png}\label{fig:firstBinPercentageEven}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 10$ for odd $n$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m10_uneven.png}\label{fig:firstBinPercentageUneven}
      \end{minipage}
\end{figure}

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 100$ for even $n$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m100_even.png}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 100$ for odd $n$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m100_uneven.png}
      \end{minipage}
\end{figure}

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 1000$ for even $n$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m1000_even.png}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 1000$ for odd $n$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m1000_uneven.png}
      \end{minipage}
\end{figure}

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 10,000$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m10000.png}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for $m = 100,000$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/binomial_Input_Solvable_m100000.png}\label{fig:lastBinPercentage}
      \end{minipage}
\end{figure}

It is easy to see that for small inputs sizes it is relevant if $n$ is even or odd for higher expected values as all curves in figure~\ref{fig:lastBinPercentage} oscillate between 0\% and 100\% for $n\ge14$.
For odd inputs the probability of a perfect partition decreases much more drastically with $m$ as for even inputs because the expected value of a single number increases with $m$.
If all values are much higher the small differences between the values can no longer even out the fact of one set having more elements than the other.
The oscillation therefore increases with increasing m.
For $n=20$ all 1000 inputs had a perfect partition for every combination of $p$ and $m$ but for $n=19$ only combinations where $mp\le300$ holds had at least one input with a perfect partition.
For expected values of up to $10^5$ it seems to be almost granted that an input of length 20 has a perfect partition if it is binomial distributed.
Even for only 12 binomial generated values more than 50\% of the inputs had a perfect partition (see figure~\ref{fig:lastBinPercentage}).
Another visible effect is the decreasing percentage with rising $p$.
This may be a direct result of the value chosen for $p$ but can also be an indirect result as the value for $p$ changes the expected value for a constant $m$.
The expected value may have an influence on the number of perfect partitions because it influences the highest value of the input.
For uniform distributed inputs Borgs \etal~showed that the coefficient of number of bits needed to encode the max value/$n$ has a huge impact on the number of perfect partitions~\cite{borgs2001phase}.
For a coefficient < 1 the probability of a perfect partition tends to 1 and for a coefficient > 1 it tends to 0.
This was only proven for the uniform distributed input, but it might also hold for a binomial distributed input.
This leads to the second experiment.\newline
In the second experiment the inputs were generated a bit differently.
Here the goal was to keep the expected value fixed for any combination of $p$ and $n$ and set the value of $m$ to $e/p$ for all $e \in \{10, 20, 30, 40, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 50000\}$ so that $E(X)=mp=e/p\cdot p=e$.
With this setup the influence of the expected value is almost isolated from the other parameters.
The probability is still linked to $p$ as $p$ also influences the variance $mp(1-p)$.\newline
Figure~\ref{fig:firstBinPercentage2} to figure~\ref{fig:lastBinPercentage2} again show the percentage of perfect inputs with different settings of $m,p,n$.
The $x$-axis is the expected value $mp$ of a single number of the input. The different graphs show the percentage for different input sizes.
It seems as if the value of $p$ has a much smaller influence than the expected value.
For a fixed expected value and a fixed input size a higher value for $p$ seems to only slightly increase the percentage of inputs with a perfect partition.
The expected value influences the percentage significantly more.
For $p=0.1, n=14$ the value decreases from 100\% at $E(X)=10$ to below 20\% at $E(X)=50000$ (figure~\ref{fig:firstBinPercentage2}).
For $p=0.9$ the percentage only drops below 50\% but still decreases by a factor of 2 (figure~\ref{fig:lastBinPercentage2}).

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for p = 0.1}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/solvability0_1.png}\label{fig:firstBinPercentage2}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for p = 0.2}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/solvability0_2.png}
      \end{minipage}
\end{figure}


% \begin{figure}[h]
%       \centering
%       \begin{minipage}[b]{0.45\textwidth}
%             \caption{Percentage of Binomial inputs with perfect partitions for p = 0.3}
%             \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/solvability0_3.png}
%       \end{minipage}
%       \hspace{0.75cm}
%       \begin{minipage}[b]{0.45\textwidth}
%             \caption{Percentage of Binomial inputs with perfect partitions for p = 0.4}
%             \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/solvability0_4.png}
%       \end{minipage}
% \end{figure}


\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for p = 0.5}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/solvability0_5.png}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Percentage of Binomial inputs with perfect partitions for p = 0.9}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/solvability0_9.png}\label{fig:lastBinPercentage2}
      \end{minipage}
\end{figure}

The last experiment showed that for $n=20$, 1000/1000 inputs had a perfect partition. This raised the question of how the amount of perfect partition changes with changing values for $m, p, n$.
Figure~\ref{fig:firstBinSolCount} to figure~\ref{fig:lastBinSolCount} show the amount of perfect partitions a binomial distribution \textasciitilde$B(m,p$) has.
For these figures 10,000 random binomial inputs with the given values for $m$ and $p$ were generated.
Each input was then tested for the number of perfect partitions it has.
The used method was again brute force to ensure correctness which was only possible due to the small input sizes.
After all runs the average values were combined in the given figures.
The value of $p$ is dependent on the picture and each value of $m \in \{10,100,1000,10000\}$ has its own graph within the figure.
The $x$-axis is the size of the input and the $y$-axis the number of perfect partitions the input has.
Notice that all graphs have a $y$-axis with a logarithmic scale.
Since the graphs are all linear the actual values rise exponentially.
The number of perfect partitions is mostly multiplied by a factor between 3 and 4 when the input size increases by 2.

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Amount of perfect partitions for $p=0.1$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/perfectPartitionCount-p0_1.png}\label{fig:firstBinSolCount}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Amount of perfect partitions for $p=0.9$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/perfectPartitionCount-p0_9.png}
      \end{minipage}
\end{figure}

\begin{figure}[h]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Amount of perfect partitions for $p=0.2$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/perfectPartitionCount-p0_2.png}
      \end{minipage}
      \hspace{0.75cm}
      \begin{minipage}[b]{0.45\textwidth}
            \caption{Amount of perfect partitions for $p=0.5$}
            \includegraphics[width=\textwidth]{figures/images/solvabilityOfInputs/perfectPartitionCount-p0_5.png}\label{fig:lastBinSolCount}
      \end{minipage}
\end{figure}

The higher the value of $m$ the closer the curves of $p$ and $1-p$ get.
For $m=1000$ and $m=10000$ the values are almost the same for every input size.
% As in this case $m$ was not defined as $e/p$ with some values for $e$ but instead some values directly for $m$, the influence of the expected values distorts the image for the lower values of $m$.
For $p=0.1, m=10$ an input with expected values of 1 seems to much more likely to have a perfect partition than an input with expected value $10\cdot0.9=9$.
With growing $m$ this has less impact.
% Figure~\ref{fig:additionalBinSolCount} shows that for equal expected values the average amount of perfect partitions is the same for $p$ and $1-p$.
% Here this fact even holds for the smaller values of $m$.

% \begin{figure}[h]
%       \caption{Amount of perfect partitions for $p=0.1$ and $p=0.9$}
%       \centering
% \includegraphics[width=0.45\textwidth]{figures/images/solvabilityOfInputs/perfectPartitionCount-p0_1Andp0_9_2.png}\label{fig:additionalBinSolCount}
% \end{figure}

So the binomial input should be easy to solve due to the exponential number of perfect partitions.
It might be harder for the smaller values of $n$ as there are only a few perfect partitions.
Due to the small number of total possibilities it should still be easy to solve for the small values of $n$ as long as the RSH is not stuck in a local optimum.
The number of iterations might be high in terms of the big-O notation but should still be small in the absolute value.
% BEGIN RESULTS

% \input{expRes/tables_2.tex}

\input{expRes/binomial.tex}
\input{expRes/geometric.tex}
\input{expRes/uniform.tex}
\input{expRes/powerlaw.tex}
\input{expRes/onemax.tex}
\input{expRes/twoThirds.tex}
% \input{expRes/mixed.tex}
% \input{expRes/overlapped.tex}
\input{expRes/mixedAndOverlapped.tex}

\section{Conclusion of empirical results}
There is no clear best algorithm for every input for PARTITION and not even a best parameter for every algorithm.
For inputs that are comparable to a linear function/OneMax for all base algorithms the parameters with the lowest mutation rate have the best runtime.
Other instances like the worst case input of C. Witt on the other hand require much higher mutation rates for the optimal performance.
Inputs generated from a powerlaw distribution showed that the optimal parameter for every algorithm is not even fixed with a specific distribution.
For inputs drawn from \textasciitilde$D^{2.75}_{50000}$ the higher mutation rates reached an optimum faster than the lower mutation rate for every algorithm variant.
If the input was drawn from \textasciitilde$D^{1.25}_{50000}$ then the fastest mutation rates for the (1+1) EA on \textasciitilde$D^{2.75}_{50000}$ distributed inputs then instead become the slowest.
So almost no general advice is possible, but a few points still hold for every input type.
The first one is the RLS being most likely to be stuck in a local optimum especially for the smaller input size.
Even if a variant of the RLS is the fastet for the bigger input sizes it is most likely to be stuck in a local optimum for $n\le100$ for most input types.
So if the input size $n\le100$ choosing the (1+1) EA or $pmut$ mutation operator is a better choice.
Another noticeable relation is that inputs that require higher mutation rates are generally easy to solve and are also solved very fast by the lower mutation rates.
A lower number of iterations also does not imply a shorter runtime in every case.
If the mutation rate $1/n$ needs only a few iterations more than $100/n$ it will still be much faster since one iteration is much shorter.
The lower mutation rates are therefore generally a better choice as they will need less time in most cases and are still rather fast if they are not the fastest.
Only if the algorithm is trapped due to its low mutation rate a higher mutation rate makes a huge difference.

Now to round this paper up there are two tables that summaries the previous results.
For each input and each algorithm the best three variants are listed in Table~\ref{table:BestAlgoVariantsTable} ordered by their average runtime.
This implies a general tendency of better algorithms but is not necessarily a complete insight as the best parameter changes depending on $n$.
Table~\ref{table:BestAlgoVariantTable} list my personal preference based on the previous results depending on the distribution and size of the input.

\begin{table}[t]
      \caption{Best algorithms variants for all researched inputs}
      \begin{tabular}{c|ccc|ccc|ccc}\label{table:BestAlgoVariantsTable}
                                                   &
            \multicolumn{3}{c|}{RLS variants}      &
            \multicolumn{3}{c|}{(1+1) EA variants} &
            \multicolumn{3}{c}{$pmut$ variants}                                                                                      \\
                                                   & 1st      & 2nd      & 3rd      & 1st     & 2nd    & 3rd    & 1st  & 2nd  & 3rd  \\\hline
            binomial                               & \RLSN[2] & \RLSN[4] & \RLSR[2] & 3$/n$   & 4$/n$  & 2$/n$  & 2.25 & 2.0  & 1.75 \\
            geometric                              & \RLSR[2] & \RLSR[3] & \RLSR[4] & 2$/n$   & 1$/n$  & 3$/n$  & 3.25 & 3.0  & 2.5  \\
            polwerlaw                              & \RLSR[4] & \RLSR[3] & \RLSN[3] & 4$/n$   & 3$/n$  & 2$/n$  & 1.5  & 1.75 & 2.0  \\
            uniform                                & \RLSN[2] & \RLSR[3] & \RLSR[4] & 3$/n$   & 2$/n$  & 4$/n$  & 2.5  & 2.0  & 2.25 \\
            linear function                        & RLS      & \RLSR[2] & \RLSR[3] & 1$/n$   & 2$/n$  & 3$/n$  & 3.5  & 3.25 & 3.0  \\
            worst case                             & \RLSN[4] & \RLSN[3] & \RLSN[2] & 100$/n$ & 50$/n$ & 10$/n$ & 1.25 & 1.5  & 1.75 \\
            combined                               & RLS      & \RLSR[2] & \RLSR[3] & 1$/n$   & 2$/n$  & 3$/n$  & 3.25 & 3.0  & 2.75 \\
      \end{tabular}
\end{table}

\begin{table}[t]
      \caption{Suggestions for the fastest algorithm on each input depending on the input size}
      \begin{tabular}{c|c|c|c|c}\label{table:BestAlgoVariantTable}
                            & $n\le100$                                         & $100<n<500$
                            & $500\le n<10,000$                                 & $10,000\le n$                            \\
            \hline
            binomial        & \multicolumn{2}{c|}{(1+1) EA $_{3/n}$}            & \multicolumn{2}{c}{\RLSN[2]}             \\
            geometric       & \multicolumn{2}{c|}{(1+1) EA $_{2/n}$}            & \multicolumn{2}{c}{$pmut_{3.25}$}        \\
            uniform         & \multicolumn{2}{c|}{(1+1) EA $_{4/n}$}            & \multicolumn{2}{c}{\RLSN[2]}             \\
            \hline
            powerlaw        & \multicolumn{4}{c}{$pmut_{1.5}$ or $pmut_{1.75}$}                                            \\
            linear function & \multicolumn{4}{c}{RLS}                                                                      \\
            worst case      & \multicolumn{4}{c}{$pmut_{1.25}$}                                                            \\
            \hline
            combined        & (1+1) EA $_{2/n}$                                 & \multicolumn{2}{c|}{$pmut_{1.25}$} & RLS \\
      \end{tabular}
\end{table}