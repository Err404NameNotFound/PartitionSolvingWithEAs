\chapter{Higher mutation rates and heavy tailed mutations}\label{ch:heavyMut}

In this chapter different ways to change the mutation rate for EAs are discussed so that they flip more bits in expectation.
For OneMax and all other linear functions the mutation rate of $p_m=1/n$ was proven to be optimal\cite{witt2013tight}.
This is not the case for every fitness function.
$\text{Jump}_k$ has a optimal mutation rate of $k/n$ and a small constant factor deviation from $k/n$ results in an increase of the runtime exponential in $\Omega(k)$\cite{doerr2017fast}.

\section{Algorithms}
For the (1+1) EA changing the expected amount of flipped bits per step can be done easily.
By changing the mutation rate $1/n$ to $c/n$ for any constant $c$ the algorithm now flips $n\cdot c/n=c$ bits in expectation.
\begin{algorithm}[bt]
      \caption{\textsc{(1+1) EA with static mutation rate}}\label{alg:EA_SM}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow x$\;
      flip every bit of $x'$ with probability $c/n$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

For the RLS it is not that simple, as the RLS chooses a random bit and flips it.
Instead of flipping c bits in every step there should be the possibility to flip different amounts of bits in every step.
The standard RLS chooses a random neighbour with Theorem~\ref{theo:OneMaxResult} one.
So the variant of the RLS could simply choose neighbours that have a Theorem~\ref{theo:OneMaxResult} larger than one.
The selection should still be uniform random to keep the idea of the RLS intact.
One possible way is to choose a random neighbour with Theorem~\ref{theo:OneMaxResult} $\le k$.
The amount of neighbours with Theorem~\ref{theo:OneMaxResult} $y$ is given by $\binom{n}{y}$.
For $k=4$, this results in $n$ neighbours with Theorem~\ref{theo:OneMaxResult} 1, $n(n-1)/2$ neighbours with Theorem~\ref{theo:OneMaxResult} 2, $n(n-1)(n-2)/6$
and $n(n-1)(n-2)(n-3)/24$.
The probability to choose a random neighbour with Theorem~\ref{theo:OneMaxResult} $y \le k$ for $k = \mathcal{O}(1)$ is given by
\[P(\text{RLS-N}_k\text{ flips }y\text{ bits}) = \frac{\binom{n}{y}}{\sum_{i=1}^k \binom{n}{i}} = \frac{\Theta(n^y)}{\sum_{i=1}^k \Theta(n^i)}
      = \frac{\Theta(n^y)}{\Theta(n^k)} = \Theta(n^{y-k}) = \Theta(\frac{1}{n^{k-y}})\]
This variant of the RLS is likely to choose a neighbour with Theorem~\ref{theo:OneMaxResult} k as the number of neighbours with hamming
distance $k$ rises with $k$ for $k \le n/2$.
The probability of flipping only one bit is $\mathcal{O}(\frac{1}{n^{k-1}})$.
For some inputs flipping only one bit might be more optimal which is rather unlikely for this variant of the RLS which will be called RLS-N from now on.
\begin{algorithm}[bt]
      \caption{\textsc{RLS-N}}\label{alg:rlsN}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow \text{uniform random neighbour of x with Theorem~\ref{theo:OneMaxResult}} \le k$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

An alternative way of changing the RLS is to first choose $y \in \{1, \dots, k\}$ uniform random and then choose a neighbour with Theorem~\ref{theo:OneMaxResult} $y$ uniform random.
Here the probability of flipping $y \le k$ bits is given by $1/k$, so the algorithm is much more likely to choose to flip only one bit.
This variant of the RLS will be referred to as RLS-R

\begin{algorithm}[bt]
      \caption{\textsc{RLS-R}}\label{alg:rlsR}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $y \leftarrow \text{uniform random value }\in \{1,\dots,k\}$\;
      $x' \leftarrow \text{uniform random neighbour of x with Hamming Distance } y$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

Both variants of the RLS change at most $k$ bits in each step and therefore only a constant amount of bits.
For the (1+1) EA the algorithm will also flip mostly $\mathcal{O}(c)$ bits which is also constant.
So neither of the new variants is likely to change up to $\mathcal{O}(n)$ bits.
Quinzan \textit{et al.} therefore introduced another mutation operator in~\cite{friedrich2018evolutionary} called $pmut_\beta$.
This operator chooses $k$ from a powerlaw distribution $D^\beta_n$ with exponent $\beta$ and maximum value $n$ and then $k$ uniform random bits are flipped.
This algorithm will mostly flip a small number of bits but occasionally up to n bits.
Distributions like this are called heavy tailed mutations because their tail is not bounded exponentially.

\section{Runtime Analysis of higher mutation rates}

The first analysed input is again the input with $w_1\ge W$.
This input is very close to a weighted OneMax and therefore more or less the OneMax equivalent for Partition.
The first analysed algorithm will be the RLS variants.

\begin{lemma}
      The \RLSR reaches the optimal solution on an input with $w_1\ge W$ for a constant $k$ in expected time $\mathcal{O}(nlogn)$
\end{lemma}
\begin{proof}
      The \RLSR chooses to flip only one bit with probability $1/k$.
      In these steps it behaves exactly the same as the standard RLS.
      The run can therefore be divided in the same two phases as in~\ref{theo:OneMaxResult}.
      The expected length of the first phase is at most $kn$ in expectation because the probability of flipping only the first is $1/kn$.
      Flipping the first bit together with other bits might be successful as well, but it will not be accepted in every case.
      In the second Phase it tries to remove every element of the bin containing $w_1$.
      In contrast to the RLS it can make steps increasing the hamming distance to the optimum.
      Steps where the first bit is not flipped can increase the Hamming distance at most by $k-1$, since the global optimum cannot change without changing $w_1$.

      For successful flip of $w_1$ after $b_E \ge \frac{W-w_1}{2}$ holds the other shifted bits must result into a volume of $2\cdot(b_E-)\frac{W-w_1}{2}$ being shifted from $b_E$ to $b_F$.
      
\end{proof}