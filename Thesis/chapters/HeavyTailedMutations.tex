\chapter{Higher mutation rates and heavy tailed mutations}\label{ch:heavyMut}

In this chapter different ways to change the mutation rate for EAs are discussed so that they flip more bits in expectation.
For OneMax and all other linear functions the mutation rate of $p_m=1/n$ was proven to be optimal\cite{witt2013tight}.
This is not the case for every fitness function.
$\text{Jump}_k$ has a optimal mutation rate of $k/n$ and a small constant factor deviation from $k/n$ results in an increase of the runtime exponential in $\Omega(k)$\cite{doerr2017fast}.

\section{Algorithms}
For the (1+1) EA changing the expected amount of flipped bits per step can be done easily.
By changing the mutation rate $1/n$ to $c/n$ for any constant $c$ the algorithm now flips $n\cdot c/n=c$ bits in expectation.
\begin{algorithm}[bt]
      \caption{\textsc{(1+1) EA with static mutation rate}}\label{alg:EA_SM}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow x$\;
      flip every bit of $x'$ with probability $c/n$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

For the RLS it is not that simple, as the RLS chooses a random bit and flips it.
Instead of flipping c bits in every step there should be the possibility to flip different amounts of bits in every step.
The standard RLS chooses a random neighbour with Hamming distance one.
So the variant of the RLS could simply choose neighbours that have a Hamming distance larger than one.
The selection should still be uniform random to keep the idea of the RLS intact.
One possible way is to choose a random neighbour with Hamming distance $\le k$.
The amount of neighbours with Hamming distance $y$ is given by $\binom{n}{y}$.
For $k=4$, this results in $n$ neighbours with Hamming distance 1, $n(n-1)/2$ neighbours with Hamming distance 2, $n(n-1)(n-2)/6$ for 3
and $n(n-1)(n-2)(n-3)/24$ for 4.
The probability to choose a random neighbour with Hamming distance $y \le k$ for $k = \mathcal{O}(1)$ is given by
\[P(\text{\RLSN}_k\text{ flips }y\text{ bits}) = \frac{\binom{n}{y}}{\sum_{i=1}^k \binom{n}{i}} = \frac{\Theta(n^y)}{\sum_{i=1}^k \Theta(n^i)}
      = \frac{\Theta(n^y)}{\Theta(n^k)} = \Theta(n^{y-k}) = \Theta(\frac{1}{n^{k-y}})\]
This variant of the RLS is likely to choose a neighbour with Hamming distance k as the number of neighbours with hamming
distance $k$ rises with $k$ for $k \le n/2$.
The probability of flipping only one bit is $\mathcal{O}(\frac{1}{n^{k-1}})$.
For some inputs flipping only one bit might be more optimal which is rather unlikely for this variant of the RLS which will be called \RLSN~from now on.
\begin{algorithm}[bt]
      \caption{\textsc{\RLSN}}\label{alg:rlsN}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $x' \leftarrow \text{uniform random neighbour of x with Hamming distance} \le k$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

An alternative way of changing the RLS is to first choose $y \in \{1, \dots, k\}$ uniform random and then choose a neighbour with Hamming distance $y$ uniform random.
Here the probability of flipping $y \le k$ bits is given by $1/k$, so the algorithm is much more likely to choose to flip only one bit.
This variant of the RLS will be referred to as \RLSR.

\begin{algorithm}[bt]
      \caption{\textsc{\RLSR}}\label{alg:rlsR}

      % Some settings
      \DontPrintSemicolon %dontprintsemicolon
      \SetFuncSty{textsc}

      % The algorithm
      \BlankLine
      choose x uniform from ${\{0,1\}}^n$\;
      \While{$x$ not optimal}
      {
      $y \leftarrow \text{uniform random value }\in \{1,\dots,k\}$\;
      $x' \leftarrow \text{uniform random neighbour of x with Hamming Distance } y$\;
      {
      \If{$f(x') \le f(x)$}
      {
            $x \leftarrow x'$\;
      }
      }
      }
\end{algorithm}

Both variants of the RLS change at most $k$ bits in each step and therefore only a constant amount of bits.
For the (1+1) EA the algorithm will also flip mostly $\mathcal{O}(c)$ bits which is also constant.
So neither of the new variants is likely to change up to $\mathcal{O}(n)$ bits.
Quinzan \textit{et al.} therefore introduced another mutation operator in~\cite{friedrich2018evolutionary} called $pmut_\beta$.
This operator chooses $k$ from a powerlaw distribution $D^\beta_n$ with exponent $\beta$ and maximum value $n$ and then $k$ uniform random bits are flipped.
This algorithm will mostly flip a small number of bits but occasionally up to n bits.
Distributions like this are called heavy tailed mutations because their tail is not bounded exponentially.

\section{Runtime Analysis of higher mutation rates}

The first analysed input is again the input with $w_1\ge W/2$.
This input is very close to a weighted OneMax and therefore more or less the OneMax equivalent for Partition.
The first analysed algorithm will be the RLS variants.

\begin{corollary}\label{corollary:W1FlipWontHappen}
      The expected time until the \RLSR[k] and \RLSN[k] flips $w_1\ge W/2$ after they reached a solution with $b_E = c\cdot\frac{W-w_1}{2}$ with $1<c<2$ is at least \(\frac{n(n-1)(c-1)}{2k^2{(k-1)}^2}\). For constant values of $c$ the expected time is $=\Omega(n^2)$.
\end{corollary}
\begin{proof}
      % For a successful flip of $w_1$ after $b_E \ge \frac{W-w_1}{2}$ holds a total volume of $2\cdot(b_E-\frac{W-w_1}{2})$ must be shifted from $b_E$ to $b_F$.
      % Otherwise the step is rejected.
      % If the algorithm moves $2\le y\le k$ elements in this step there are $y-1$ bits left to shift the volume from $b_E$ to $b_F$.
      % The object with the highest shifted volume must have a volume of at least $2\cdot(b_E-\frac{W-w_1}{2})/(y-1)$ because otherwise there is not enough total volume shifted from $b_E$ to $b_F$.
      % For $b_E = c\cdot\frac{W-w_1}{2}$ with $1<c<2$ this leads to at most \(d\le\frac{W-w_1}{2(c\frac{W-w_1}{2}-\frac{W-w_1}{2})/(y-1)}=\frac{(W-w_1)(y-1)}{(W-w_1)(c-1)}=\frac{y-1}{c-1}\) objects with the given weight.
      % % For $c=1.2$, which means $b_E = 0.6(W-w_1)$, this evaluates to $d\le5(y-1)$ which is constant for any constant k.
      % The probability for such a step, given the algorithm flips $y$ bits, is therefore at most
      % \[
      %       \frac{\binom{1}{1}\cdot\binom{d}{1}\cdot\binom{n-2}{y-2}}{\binom{n}{y}}
      %       =\frac{d\frac{(n-2)!}{(n-2-y+2)!\cdot (y-2)!}}{\frac{n!}{(n-y)!\cdot y!}}
      %       =\frac{d\cdot(n-2)!\cdot(n-y)!\cdot y!}{n!\cdot(n-y)!\cdot(y-2)!}
      %       =\frac{dy(y-1)}{n(n-1)}
      %       \]
      % The expected time for such a step to happen is at least
      % \[
      %       \frac{1}{\probP(y\text{ bits are flipped})}\cdot\frac{n(n-1)}{dy(y-1)}
      %       \ge\frac{1}{1}\cdot\frac{n(n-1)(c-1)}{y{(y-1)}^2}
      %       \ge\frac{n(n-1)(c-1)}{k{(k-1)}^2}
      %       \]
      Using Lemma~\ref{lemma:W1FlipWontHappen}, the upper bound of $y\le k$ and the fact that both algorithms flip at most $k$ bits leads to the first part
      \[ {(k\cdot\frac{2y{(y-1)}^2}{n(n-1)(c-1)})}^{-1}=\frac{n(n-1)(c-1)}{2ky{(y-1)}^2} \le\frac{n(n-1)(c-1)}{2k^2{(k-1)}^2}\]
      $k$ is constant and if c is constant too, this leads to expected time $=\Omega(n^2)$ for a flip of the first bit if $b_E = c\cdot\frac{W-w_1}{2}$ with $1<c<2$ which concludes the second statement.
\end{proof}

\begin{lemma}\label{lemma:RLSRoneMaxInput}
      The \RLSR[k] reaches the optimal solution on an input with $w_1\ge W/2$ in expected time $\mathcal{O}(n\log{}n)$
\end{lemma}
\begin{proof}
      This proof is similar to proof for the (1+1) EA in Theorem~\ref{theo:OneMaxResult} and is divided in the same two parts.
      The first part is proving the \RLSR~does not flip the first bit in expected time $\mathcal{O}(n\log{}n)$ after some time $T$ has passed.
      After time $T$ the algorithm then minimises the linear function in expected time $\mathcal{O}(n\log{}n)$ before the first bit is flipped and the progress of the linear function might be reset.
      After expected time $(2\lceil kn\ln(2/0.4)\rceil)=2\lceil kn\ln(5)\rceil\le2\lceil 1.61\cdot kn\rceil\le4kn$ the \RLSR[k] reaches a solution of \(f(x)\le w_1+0.4(W-w_1)\) (Lemma~\ref{lemma:CWittRefined}).
      This means \(b_E \ge 0.6(W-w_1) = 1.2\cdot\frac{W-w_1}{2}\).
      With Lemma~\ref{corollary:W1FlipWontHappen} the expected time of at least \(\frac{n(n-1)(c-1)}{k^2{(k-1)}^2}\frac{n(n-1)}{5k^2{(k-1)}^2}=\Omega(n^2)\) for a successful flip of $w_1$ follows. This concludes the first part\newline
      % So if the \RLSR~manages to find the optimum in time $\mathcal{O}(n\log{}n)$ the first bit won't be flipped and the global optimum does not change for the rest of the run in expectation.\newline
      The \RLSR~can be seen as an unbiased unary black box algorithm with a sequence $\mathcal{D}=(p_1,\dots,p_n)$ where $p_i=1/k$ for $1\le i\le k$ and $p_i=0$ otherwise.
      The mean of this sequence is \(\mathcal{X}=\sum_{i=1}^{n}{\probP(X=i)i}=\sum_{i=1}^{k}{\frac{i}{k}}=\frac{1}{k}\cdot\sum_{i=1}^{k}{i}=\frac{k+1}{2}\).
      For any constant value $k$ both the probability $p_1=\frac{1}{k}=\Theta(1)$ and mean $\mathcal{X}=\frac{k+1}{2}=O(1)$ meet the conditions of Theorem 1 in~\cite{doerr2023tight} and therefore the \RLSR~optimises the linear function in expected time \((1+o(1))\frac{1}{p_1}n\ln n=(1+o(1))kn\ln n\).
      Applying the same arguments as in Theorem~\ref{theo:OneMaxResult} this results in a probability of not flipping $w_1$ after expected time $4kn$ of at least
      \begin{gather}\nonumber
            1-\frac{(1+o(1))kn\ln n\cdot5k^2{(k-1)}^2}{n(n-1)}
            =1-\frac{(1+o(1))5k^5\ln n}{n-1}
            =1-\frac{o(k^5n^{0.5})}{n-1}
            =1-\frac{1}{o(\sqrt{n})}
      \end{gather}
      The expected time of minimising the linear function then is
      \[\frac{1}{1-\frac{1}{o(\sqrt{n})}}\cdot(1+o(1))kn\ln n=\frac{1}{1-o(1)}\cdot(1+o(1))kn\ln n=\Theta(n\log{}n)\]
      The total expected runtime therefore is at most $4kn+\frac{1+o(1)}{1-o(1)}\cdot kn\ln n=\Theta(n\log{}n)$.

      % The run can be divided in the same two phases as in Theorem~\ref{theo:OneMaxResult}.
      % The expected length of the first phase is at most $2kn$ in expectation because the probability of flipping only the first bit is $1/kn$.
      % Flipping the first bit together with other bits might be successful as well, but it will not be accepted in every case.\newline
      % In the second Phase the \RLSR~tries to remove every element of the bin containing $w_1$.
      % In contrast to the RLS it can make steps increasing the hamming distance to the optimum once the algorithm is in phase 2.
      % Steps where the first bit is not flipped can increase the Hamming distance at most by $k-1$, since the global optimum cannot change without changing $w_1$ and moving all $k$ elements in the wrong direction will always be rejected.

      % Now the run is again seen as a optimisation of a linear function 

      % The \RLSR[2] chooses to flip only one bit with probability $1/2$.
      % In these steps it behaves exactly the same as the standard RLS.\
      % For $k=2$ a step moving two elements cannot increase the Hamming distance if $w_1$ is not flipped.
      % Using a fitness level argument the expected running time of phase 2 if steps flipping the first bit are always rejected is at most
      % \[
      %       E(T_{Phase~2})
      %       \le \sum_{i=1}^{n}{\frac{kn}{i}}
      %       = kn\cdot\sum_{i=1}^{n}{i}
      %       % = knH(n) 
      %       \le kn(\ln(n)+1)
      %       = kn\ln(n)+kn
      %       = \mathcal{O}(n\ln n)
      % \]
      % So the algorithm reaches an optimal solution in expected time $\mathcal{O}(n\ln n)$ whereas a second flip of $w_1$ only happens in expected time $\Omega(n^2)$.
      % The algorithm therefore reaches an optimal solution in expected time
      % \[E(T_{Phase~1}) + E(T_{Phase~2})
      %       \le 4kn + kn\ln(n)+kn
      %       = kn\ln(n)+5kn
      %       = \mathcal{O}(n\ln n)\]
\end{proof}

% \begin{lemma}
%       The \RLSN[k] reaches the optimal solution on an input with $w_1\ge W/2$ in expected time TODO
% \end{lemma}
% \begin{proof}
%       This proof is similar to proof for the (1+1) EA in Theorem~\ref{lemma:RLSRoneMaxInput} and is divided in the same to parts.

%       The \RLSR~can be seen as an unbiased unary black box algorithm with sequence $\mathcal{D}=(1/k, \dots,1/k,0,\dots,0)$ where $p_i=1/k$ for $1\le i\le k$ and $p_i=0$ otherwise.
%       The mean of this sequence is \(\mathcal{X}=\sum_{i=1}^{n}{\probP(X=i)i}=\sum_{i=1}^{k}{\frac{i}{k}}=\frac{1}{k}\cdot\sum_{i=1}^{k}{i}=\frac{k+1}{2}\).
%       For any constant value $k$ both the probability $p_1=\frac{1}{k}=\Theta(1)$ and mean $\mathcal{X}=\frac{k+1}{2}=O(1)$ meet the conditions of Theorem 1 in~\cite{doerr2023tight} and therefore opmtimise the linear function in expected time \((1+o(1))\frac{1}{p_1}n\ln n=(1+o(1))kn\ln n\).
%       The total expected runtime therefore is at most $4k+(1+o(1))kn\ln n=\Theta(n\log{}n)$.
% \end{proof}