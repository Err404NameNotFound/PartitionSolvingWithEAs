\section{Binomial distributed values}\label{Sec:BinomialRuntime}
In the following sections the performance of the different algorithms is tested for different kinds of inputs.
The exact distributions of the input are explained separately in each subsection.
The procedure for each comparison is always the same. A random input is generated according to the distribution and then solved by every algorithm.
All algorithms had the same two stopping conditions.
The first was reaching a perfect partition and the second was taking more than $10 \cdot n\ln(n)$ steps.
For the lower values of $n$ the step limit of 100,000 was used instead.
For $n=20$ giving the algorithm only 600 steps is rather small.
In some cases the smaller inputs are even more difficult to solve.
Most modern computer should be able to handle 100,000 iterations in a short amount of time anyway.
So the minimum step limit of 100,000 seemed reasonable.
If either of these conditions was met, the algorithm returned its current best solution.
This step is repeated 10,000 times.
The results are presented in a table containing multiple statistics for each algorithm over all 10,000 runs.
The data is explained in the table below.

\begin{tabular}{c|l}
      column name     & meaning                                                         \\ \hline
      algo type       & type of algorithm (RLS, \RLSN, \RLSR, (1+1) EA or pmut)         \\
      algo param      & parameter of the algorithm or '-' if it is the standard variant \\
      avg mut/change  & average \#bits flipped for iterations leading to an improvement \\
      avg mut/step    & average \#bits flipped for any iteration                        \\ \hline
      total avg count & average \#iterations for all runs                               \\
      avg eval count  & average \#iterations of runs returning an optimal solution      \\
      max eval count  & maximum \#iterations of runs returning an optimal solution      \\
      min eval count  & minimum \#iterations of runs returning an optimal solution      \\ \hline
      % fails           & number of runs that did not find an optimal solution            \\
      fail ratio      & ratio of unsuccessful runs to all runs                          \\
      avg fail dif    & average value of $f(x)-W/2$ for `non-optimal' solutions         \\ \hline
      p-value         & the p-value of a Mann-Whitney-U-Test between two columns        \\
\end{tabular}

EA-SM refers to the (1+1) EA with a static mutation rate $c/n$ that is fixed for the run.
Sometimes every algorithm managed to find an optimum in each run.
To avoid redundancy and shorten the thesis the rows `total avg count' and `avg fail dif' are not shown in the table in those cases.
The p-value between two columns refers to the p-value of a Mann-Whitney-U-Test between the algorithms in the columns left and right to the value.
$H_0$ is: the algorithm on the left is not faster than the algorithm on the right.
$H_1$ in this case then is: the algorithm on the left is better than algorithm on the right.
So if the p-value is below 0.05 the algorithm on the left is statistically significant faster.\newline
Firstly the different variants of the RLS are compared with values of $k \in\{2,3,4\}$, then the performance of the (1+1) EA with static mutation rate $c/n$ with $c \in\{1,2,3,5,10,50,100\}$ and lastly the performance of the $pmut_\beta$ mutation operator with the parameter $\beta \in \{1.25, 1.5, \dots, 2.75,3.0,3.25\}$.
Afterwards there is a comparison for multiple input sizes of the best algorithms because the best algorithm is often dependent on the size of the input.
Normally there are three tables for each input.
The first states how often the algorithms did not find an optimal solution for the different input sizes (`fails in 10000 runs' in top left cell).
The second gives their average runtime for the successful runs (`avg' in top left cell) and the last the runtime for all runs (`total avg' in top left cell).
The last two tables differ in the unsuccessful runs.
Often the algorithm is stuck in a local optimum it won't leave in reasonable time or even never for variants of the RLS.\
In these cases the step limit is the deciding factor on how big the penality for this run is.
So neither of the two average values alone is enough to give a complete insight on the performance.
Sometimes a variant of the RLS is much faster than the other algorithms for a specific input but is also the only algorithm to get stuck in a local optimum.
This creates the possibility to start the RLS variant with a low step limit and switch to the (1+1) EA if the RLS variant does not return an optimal solution.
Giving both tables for the different average values might help with this decision.
There are also some cases where no perfect partition exists.
If these algorithms are used in practice this will be unclear in general as well.
To make this experiment closer to a real world scenario the value of the best possible solution was not calculated beforehand.
So in some runs it might seem like an algorithm did not find an optimal solution, although it has.
The solution was just not acknowledged as optimal, because it wasn't a perfect partition.

The first analysed inputs are inputs following a binomial distribution \textasciitilde$B(m,p)$ as those inputs have been researched in the previous subsection.
The results showed that the expected value of a single number is the main driver for the amount of perfect partitions the input has.
The results also suggested the inputs tend to have more perfect partitions if the expected value is lower.
The more perfect partitions an input has relative to the number of all possible partitions, the more likely the different RSHs are to find one of those.
Therefore researching inputs with higher expected values seems more interesting but generating higher values takes more time with a random number generator that needs $\mathcal{O}(mp)$ time.
To keep the time for generating one set of numbers reasonable the values chosen for all tests are $m=10000, p=0.1, n=10000$ with the expected value for a single number being $mp=1000$.
Figure~\ref{fig:binDistExample} shows a random binomial distributed input of length $n=10000$.
For this input type almost every time all elements were sharply concentrated around the expected value with all values being at $1000\pm200$ (for Figure~\ref{fig:binDistExample} even closer at $1000\pm121$).
So after reaching a difference to the optimum of below $(1000-200)/2=400$ the algorithm can no longer achieve an improvement by flipping a single bit (Corollary~\ref{cor:RLSStuck})

\begin{figure}[h]
      \caption{Distribution of a random binomial input}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/binomialDistributionForN10000p0_1.png}\label{fig:binDistExample}
\end{figure}
\subsection{RLS Comparison}
\input{tables/binomial/rls_compare.tex}
The \RLSN[2] seems to perform the best as it mostly switches two elements which works great for binomial distributed inputs.
The same algorithm with $b=4$ performs a bit worse but still good as switching 4 elements can be beneficial as well.
The variant of \RLSN[3] on the other hand does not reach the optimal solution in 20\% of the inputs with an average difference of 1.
It also needs 1000 times more iterations to find an optimum on average compared to the best algorithm \RLSN[2].
The \RLSR[s] variants behave mostly the same with $s=2$ being the best, followed by $s=4$ and $s=3$.
In this case the variant of $s=3$ is by far not as bad as for the \RLSN[3] because the probability of flipping 2 bits is still 1/3 compared to $\mathcal{O}(n^{-1})$ for the \RLSN[3].
The \RLSR~all seem to be a good option for binomial inputs with values of $k\in\{2,3,4\}$.
The standard RLS on the other hand performs by far the worst as it only moves one element per step.
It only managed to reach the optimal solution 13 times for 10,000 different inputs.
The average number of iterations for those inputs was only 103 so the RLS likely had a good initialisation with a few lucky steps leading directly to the optimum.
For all other cases the average difference between the bins was 246 which is close to the median of the values from 0 to $(1000-100)/2=450$.
This is likely due to the RLS being unable to improve the solution once the current solution has a difference below half of the lowest value (Corollary~\ref{cor:RLSStuck}).
So the proof of Lemma~\ref{lemma:RLSBadBinomial} is missing, but it probably still holds.
\subsection{(1+1) EA Comparison}
For the (1+1) EA the best static mutation rate seems to be $3/n$.
The probability of flipping 2 or 4 bits as $n$ goes to infinity for mutation rate $1/n$ approaches $13/24e\approx 0.199$, for $2/n$ approaches $8/3e^2\approx 0.361$, for $3/n$ approaches $63/8e^3\approx 0.392$, for $4/n$ approaches $56/3e^4\approx 0.342$ and for $5/n$ approaches $77/2e^5\approx 0.259$.
So the highest probability has $c=3$, followed by $c=4$ and $c=2$ then $c=5$ and lastly $c=1$.
For higher values of $c$ the probability decreases further as the expected number of flipped bits is $c$ for mutation rate $c/n$.

\input{tables/binomial/ea_compare.tex}
The static mutation rate $3/n$ seems to perform the best with both $4/n$ and $2/n$ being a close second place.
The next best values are $5/n$ and the standard $1/n$.
From then on the number of iterations rises monotonically with rising mutation rate.
The higher mutation rates perform drastically worse but the still find a solution within the limit as opposed to the standard RLS.\
\subsection{pmut Comparison}
\input{tables/binomial/pmut_compare.tex}
For the $pmut_\beta$ mutation operator the choice of $\beta$ seems to be much more insignificant than for the RLS or (1+1) EA.\
Here all values perform comparably good with only the value of $\beta = 1.25$ having a clear performance difference compared to next best value.
All values of $\beta$ reach an optimal solution in every case.
The worst variant of the $pmut_\beta$ operator still performs much better than the worst value for the (1+1) EA and even better than the worst RLS variant.
There is no clear best algorithm because there is no statistically significant difference between the best to values.
\subsection{Comparison of the best variants}
Looking at the previous tables for this setting of $m=10000, p=0.1, n=10000$ the \RLSN[2] performs better than the  (1+1) EA and $pmut_\beta$ for all values of $c/n$ and $\beta$ by a factor of at least 2.
This is likely from the fact that this version of the RLS flips almost only two bits which seems to be close to optimal for this kind of input.
There are many values close to the expected value which can be switched to make small adjustments to the fitness value.
To further investigate which algorithm performs best on all binomial distributed inputs now a comparison with different input lengths follows.
The parameters of the distribution were not changed.\newline
The following table shows the number of runs in which the algorithms did not find an optimal solution within $\max\{100,000;10 \cdot n\ln(n)\}$ steps for different input sizes of $n$.
For $n\le1382$ the step limit is 100,000 and for the bigger values of $n$ it is $10 \cdot n\ln(n)$.

\input{tables/binomial/multipleN_fails.tex}

The (1+1) EA and $pmut_\beta$ always reach an optimal solution but the RLS variants do not.
The RLS variants that can only flip two bits per step perform significantly worse for very small inputs.
They are probably more likely to get stuck in a local optimum where a step flipping 4 bits or more would be necessary.
So the \RLSN[2] does perform better for larger inputs but is much more likely to get stuck in a local optimum for smaller $n$ values.
The next table contains the average number of iterations the algorithm needed to find an optimal solution for all runs where the algorithms managed to find an optimal solution.
Here it still looks like the \RLSN[2] finds the solution with the lowest amount of steps, because the cases where the algorithm is stuck in a local optima are not contained in this table.

\input{tables/binomial/multipleN_avg.tex}

The next table contains the overall average amount of iterations for every run.
So runs where no optimal result was found add 100,000 to the sum of all iterations for small values of $n$.

\input{tables/binomial/multipleN_totalAvg.tex}

Here the \RLSN[2] is only the best algorithm for values of $n \ge 500$.
Below this bound choosing the (1+1) EA with static mutation rate $3/n$ is a safer choice as the (1+1) EA reaches an optimal solution for every input (in this experiment).
