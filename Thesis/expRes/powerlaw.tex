\section{powerlaw distributed inputs}
This distribution has mostly small values, but occasionally it also generates bigger values.
The lower the parameter the higher the values get and also the amount of big values increases.
For a parameter of $\beta=2.75$ and a maximum value of 10,000 the distribution looks like in Figure~\ref{fig:powerDistExample1}.
All values are rather small and less than 100, also half of the values are one.
So this input seems rather easy for $\beta=2.75$.

\begin{figure}[h]
      \caption{Distribution of a random powerlaw input with $\beta=2.75$}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/powerlaw_-2_75.png}\label{fig:powerDistExample1}
\end{figure}

For a value of $\beta=1.25$ the distribution looks a bit different.
There are less small values close to one and instead also big values even over 1000.
Figure~\ref{fig:powerDistExample2} is cropped to get a more clear view for the smaller values.
The higher values mostly occurred 0 to 2 times.
The highest value 9948 occurred only once.
Researching inputs like this should be more interesting which is why $\beta=1.25$ was chosen for the experiment.
To give a better view on this type of input there is also a table for $\beta=2.75$ at the evaluation of the (1+1) EA.\
The results for the other algorithms were mostly the same, but these are not shown here for better readability.

\begin{figure}[h]
      \caption{Distribution of a random powerlaw input with $\beta=1.25$}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/powerlaw_-1_25.png}\label{fig:powerDistExample2}
\end{figure}
\subsection{RLS Comparison}
\input{tables/powerlaw/rls_compare.tex}

The input is even easier to solve for the RLS variants than the binomial distributed inputs.
There is no clear tendency and all algorithms have a rather equal runtime.
All algorithm manage to find an optimal solution in every run.
\subsection{(1+1) EA Comparison}
The first table shows the results for parameter $\beta=2.75$

\input{tables/powerlaw/ea_compare2_75.tex}

For $\beta=2.75$ the results are much different from $\beta=1.25$.
Until $p_m\le50/n$ the speed of convergence increases but at $p_m=100/n$ the speed decreases again.
The optimal value seems to be somewhere around $p_m=50/n$.
The (1+1) variants are generally faster than all RLS variants when comparing the maximum number of iterations.
For mutation rates $3/n\le p_m \le 100/n$ the (1+1) EA is also faster on average.
The next table shows the results for a powerlaw distribution with $\beta=1.25$.

\input{tables/powerlaw/ea_compare.tex}

With this setting the optimal value is shifted to somewhere around $p_m=4/n$.
The higher mutation rates perform drastically slower with $p_m=100/n$ being 500 times slower than the optimal value.
The speed of convergence is sometimes even to slow find an optimal solution in time $10\cdot n\ln(n)$.
So the parameter of the distribution does change the optimal parameter for the Evolutionary Algorithm solving the input.
The same was true for the RLS and also for $pmut$.
\subsection{pmut Comparison}
\input{tables/powerlaw/pmut_compare.tex}

The optimal value here seems to be somewhere around $\beta=1.5$ instead of $\beta =1.25$ for inputs from \textasciitilde$D^{2.75}_{20.000}$. 
So the optimal value is only lightly smaller in comparison to the (1+1) EA where the optimal value almost change from one side of the spectrum to the other.

\subsection{Comparison of the best variants}
The \RLSR[4] performs equally good as the (1+1) EA variant with $p_m=4/n$, but both are slower than $pmut_{1.5}$.
Choosing any algorithm should solve the input quite fast if the parameter of the algorithm is somewhat close to the optimum.

\input{tables/powerlaw/multipleN_fails.tex}

The RLS is once again the algorithm that is the most likely to be stuck in a local optimum.
Compared to the other algorithms it is not as drastic as for the binomial input for example.
Only for $n<500$ the algorithms do not find a global optimum in every run.
The setting of the parameter and the choice of the algorithm almost doesn't affect the amount of runs without an optimal result.
This type of input is probably easy to solve if it has a perfect partition.
The two stopping conditions where a step limit and finding a perfect partition or a partition with difference of one between the two bin for uneven $n$.
So in 78\% of the runs the algorithms might have found an optimal solution, but the stopping conditions did not trigger as the found solutions were no perfect partitions.

\input{tables/powerlaw/multipleN_avg.tex}

Looking at the time the algorithms needed on average the runs that hit the step limit could have possibly been no failed runs.
The easiest are inputs with size $n=500$.
For smaller values of $n$ the algorithms sometimes fail and even in a good run they need more iterations to find an optimal solution.
Due to the increasing size of the input the algorithms need more time for the bigger values.

\input{tables/powerlaw/multipleN_totalAvg.tex}

$pmut_{1.75}$ and $pmut_{1.5}$ are not only the best variant for the bigger values of n but also for smaller inputs as well.
They are the least likely to be stuck in a local optimum, and are  also the fastest if they reach a global optimum.
