\section{powerlaw distributed inputs}
This distribution has mostly small values, but occasionally it also generates bigger values.
The lower the parameter the higher the values get and also the amount of big values increases.
For a parameter of $\beta=2.75$ and a maximum value of 10,000 the distribution looks like in Figure~\ref{fig:powerDistExample1}.
All values are rather small and less than 100, also half of the values are one.
So this input seems rather easy for $\beta=2.75$.

\begin{figure}[h]
      \caption{Distribution of a random powerlaw input with $\beta=2.75$}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/powerlaw_-2_75.png}\label{fig:powerDistExample1}
\end{figure}

For a value of $\beta=1.25$ the distribution looks a bit different.
There are less small values close to one and instead also big values even over 1000.
Figure~\ref{fig:powerDistExample2} is cropped to get a more clear view for the smaller values.
The higher values mostly occurred 0 to 2 times.
The highest value 9948 occurred only once.
Researching inputs like this should be more interesting which is why for the experiment $\beta=1.25$ was chosen.
To give a better view on this type of input there is also a table $\beta=2.75$ at the evaluation of the (1+1) EA.\
The results for the other algorithms were mostly the same, but these are not shown here for better readability.

\begin{figure}[h]
      \caption{Distribution of a random powerlaw input with $\beta=1.25$}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/powerlaw_-1_25.png}\label{fig:powerDistExample2}
\end{figure}
\subsection{RLS Comparison}
\input{tables/powerlaw/rls_compare.tex}

The input is even easier to solve for the RLS variants than the binomial distributed inputs.
There is no clear tendency and all algorithms have a rather equal runtime and also every algorithm manages to find an optimal solution in every run.
\subsection{(1+1) EA Comparison}
The first table shows the results for parameter $\beta=2.75$

\input{tables/powerlaw/ea_compare2_75.tex}

Here the same rule holds for the RLS to some extent.
Until $p_m\le50/n$ the speed of convergence increases but at $p_m=100/n$ the speed decreases again.
The optimal value seems to be somewhere around $p_m=50/n$.
The (1+1) variants are generally faster than all RLS variants when comparing the maximum number of iterations.
For mutation rates $3/n\le p_m \le 100/n$ the (1+1) EA is also faster on average.
The next table shows the results for a powerlaw distribution with $\beta=1.25$.

\input{tables/powerlaw/ea_compare.tex}

With this setting the optimal value is shifted to somewhere around $p_m=4/n$.
The higher mutation rates perform drastically slower with $p_m=100/n$ being 500 times slower than the optimal value.
The speed of convergence is sometimes even to slow find an optimal solution in time $10\cdot n\ln(n)$.
\subsection{pmut Comparison}
For pmut the same holds as for the RLS.\
The more bits the algorithms flips on average the better the performance on average.
Surprisingly the performance in the worst runs behaves inverted.
The fewer bits the algorithm flips on average the more stable the search becomes.
This might be caused by the really large amount of bits flipped for the lower values.

\input{tables/powerlaw/pmut_compare.tex}

The optimal value here seems to be somewhere around $\beta=1.5$, so only lightly smaller in comparison to the (1+1) EA where the optimal value almost change from one side of the spectrum to the other.
Here the inverted stability of the search does not occur.
The variants that take longer on average tend to also take longer in their worst runs.

\subsection{Comparison of the best variants}
The first table again shows the results for parameter $\beta=2.75$

\input{tables/powerlaw/best.tex}

The ranking follows the amount of bits the algorithms flip on average per step.
$pmut_{1.25}$ manages to find the solution in just 56 iterations on average.
The (1+1) EA with $p_m=50/n$ is slower than $pmut_{1.25}$ but instead has a lower value for the maximum number of iterations.
Both options seem fine.
Even the \RLSN[4] is still very fast for the powerlaw distributed input with $\beta = 2.75$.
For $\beta = 1.25$ the results are a bit different.

The \RLSR[4] now performs equally good as the (1+1) EA variant with $p_m=4/n$, but is still slower than $pmut_{1.5}$.
As the first inputs were less difficult to solve than the inputs with $\beta = 1.25$ the second value was chosen for the evaluation of smaller input sizes.

\input{tables/powerlaw/multipleN_fails.tex}

The RLS is once again the algorithm that is the most likely to be stuck in a local optimum.
Compared to the other algorithms it is not as drastic as for the binomial input for example.
Only for $n<500$ the algorithms do not find a global optimum in every run.
The setting of the parameter almost doesn't affect the amount of runs without an optimal result.
The main differences are between the different algorithms themselves.
This type of input is probably easy to solve if it has a perfect partition.
The two stopping conditions where a step limit and finding a perfect partition or a partition with difference of one between the two bin for uneven $n$.
So in 80\% of the runs the algorithms might have found an optimal solution, but the stopping conditions did not trigger as the solutions where not close to a perfect partition.

\input{tables/powerlaw/multipleN_avg.tex}

Looking at the time the algorithms needed on average the runs that hit the step limit could have possibly been no failed runs.
The easiest are inputs with size $n=500$.
For smaller values of $n$ the algorithms sometimes fail and even in a good run they need more iterations to find an optimal solution.
Due to the increasing size of the input the algorithms need more time for the bigger values.

\input{tables/powerlaw/multipleN_totalAvg.tex}

$pmut_{1.75}$ is not only the best variant for the bigger values of n but also for smaller inputs as well.
It is the least likely to be stuck in a local optimum, and it is also the fastest if it reaches a global optimum.
