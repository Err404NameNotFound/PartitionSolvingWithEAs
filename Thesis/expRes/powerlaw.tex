\section{powerlaw distributed inputs}
This distribution has mostly small values, but occasionally it also generates bigger values.
The higher (absolute lower) the parameter the higher the values get and also the amount of big values increases.
For a parameter of $\beta=-2.75$ and a maximum value of 10,000 the distribution looks like in Figure~\ref{fig:powerDistExample1}.

\begin{figure}[h]
      \caption{Distribution of a random powerlaw input with $\beta=-2.75$}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/powerlaw_-2_75.png}\label{fig:powerDistExample1}
\end{figure}

For a value of $\beta=-1.25$ the distribution looks a bit different.
There are less small values close to one and instead also big values even over 1000.
Figure~\ref{fig:powerDistExample2} is cropped to get a more clear view for the smaller values.
The higher values mostly occurred 0 to 2 times.
The highest value 9948 occurred only once.

\begin{figure}[h]
      \caption{Distribution of a random powerlaw input with $\beta=-1.25$}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/powerlaw_-1_25.png}\label{fig:powerDistExample2}
\end{figure}
\subsection{RLS Comparison}
The following table lists the results for the RLS for inputs that are chosen from a powerlaw distribution with $\beta=-2.75$.


\input{tables/powerlaw/rls_compare.tex}

Due to the many small values and all values being relatively small in general the higher mutation rates perform better.
So the algorithms are ranked by the amount of bits they flip in an average step.
For this parameter the input is relatively easy as all variants manage to find an optimal solution in 400 steps on average for an input size of 20,000.
The next table lists the results for $\beta=-1.25$ and an input size of 10,000.

\input{tables/powerlaw/rls_compare_2.tex}

The input is still easy to solve for the RLS variants, but the concept of the more bits flipped the better does not hold any more.
So the optimal algorithm parameter for an input from a specific distribution is not fixed for the complete distribution.
Instead the optimal value can change even within the distribution if the parameters of the distribution change.
Generally the RLS-R variants seem to perform good for the different parameters of the distribution especially with increasing value of $k$.
\subsection{(1+1) EA Comparison}
The first table again shows the results for parameter $\beta=-2.75$

\input{tables/powerlaw/ea_compare.tex}

Here the same rule holds for the RLS to some extent.
Until $p_m\le50/n$ the speed of convergence increases but at $p_m=100/n$ the speed decreases again.
The optimal value seems to be somewhere around $p_m=50/n$.
The (1+1) variants are generally faster than all RLS variants when comparing the maximum number of iterations.
For mutation rates $3/n\le p_m \le 100/n$ the (1+1) EA is also faster on average.
The next table shows the results for a powerlaw distribution with $\beta=-1.25$.

\input{tables/powerlaw/ea_compare_2.tex}

With this setting the optimal value is shifted to somewhere around $p_m=4/n$.
The higher mutation rates perform drastically slower with $p_m=100/n$ being 500 times slower than the optimal value.
The speed of convergence is sometimes even to slow find an optimal solution in time $10*n\ln(n)$.
\subsection{pmut Comparison}
The first table again shows the results for parameter $\beta=-2.75$

\input{tables/powerlaw/pmut_compare.tex}

For pmut the same holds as for the RLS.
The more bits the algorithms flips on average the better the performance on average.
Surprisingly the performance in the worst runs behaves inverted.
The fewer bits the algorithm flips on average the more stable the search becomes.
This might be caused by the really large amount of bits flipped for the lower values.

\input{tables/powerlaw/pmut_compare_2.tex}

The optimal value here seems to be somewhere around $\beta=-1.5$, so only lightly smaller in comparison to the (1+1) EA where the optimal value almost change from one side of the spectrum to the other.
Here the inverted stability of the search does not occur.
The variants that take longer on average tend to also take longer in their worst runs.

\subsection{Comparison of the best variants}
The first table again shows the results for parameter $\beta=-2.75$

\input{tables/powerlaw/best.tex}

The ranking follows the amount of bits the algorithms flip on average per step.
$pmut_{-1.25}$ manages to find the solution in just 56 iterations on average.
The (1+1) EA with $p_m=50/n$ is slower than $pmut_{-1.25}$ but instead has a lower value for the maximum number of iterations.
Both options seem fine.
Even the $RLS-N_4$ is still very fast for the powerlaw distributed input with $\beta = -2.75$.
For $\beta = -1.25$ the results are a bit different.

\input{tables/powerlaw/best_2.tex}

The $RLS-R_4$ now performs equally good as the (1+1) EA variant with $p_m=4/n$, but is still slower than $pmut_{-1.5}$.
As the first inputs were less difficult to solve than the inputs with $\beta = -1.25$ the second value was chosen for the fine evaluation.

\input{tables/powerlaw/multipleN_fails.tex}

The RLS is once again the algorithm that is the most likely to be stuck in a local optimum.
Compared to the other algorithms it is not as drastic as for the binomial input for example.
Only for $n<500$ the algorithms do not find a global optimum in every run.
The setting of the parameter almost doesn't affect the amount of runs without an optimal result.
The main differences are between the different algorithms themselves.
This type of input is probably easy to solve if it has a perfect partition.
The two stopping conditions where a step limit and finding a perfect partition or a partition with difference of one between the two bin for uneven $n$.
So in 80\% of the runs the algorithms might have found an optimal solution, but the stopping conditions did not trigger as the solutions where not close to a perfect partition.

\input{tables/powerlaw/multipleN_avg.tex}

Looking at the time the algorithms needed on average the runs that hit the step limit could have possibly been no failed runs.
The easiest are inputs with size $n=500$.
For smaller values of $n$ the algorithms sometimes fail and even in a good run they need more iterations to find an optimal solution.
Due to the increasing size of the input the algorithms need more time for the bigger values.

\input{tables/powerlaw/multipleN_totalAvg.tex}

$pmut_-1.75$ is not only the best variant for the bigger values of n but also for smaller inputs as well.
It is the least likely to be stuck in a local optimum, and it is also the fastest if it reaches a global optimum.
