\section{Geometric distributed values}
For the geometric distribution the chosen default value is $p=0.001$.
This results in an expected value of 1000 which is the same as for the binomial distribution in the last subsection.
This should make the results more comparable.
The maximum value is theoretically not limited but for the implementation in Java the maximum value was set to the maximum value of a long value $= 2^{63}-1 = 9,223,372,036,854,775,807$.
Without this maximum the value might overflow and instead be negative with high absolute value.
Figure~\ref{fig:geoDistExample} shows a random geometric distributed input.
The span of all values is way higher than for the binomial distribution, although they have same expected value.
Here the values are not in the interval $[800,1200]$ but rather between 0 and 9000.
The theoretical limitation of the values being at most $2^{63}-1$ seems to not have an influence on the results.
The geometric distribution does not only have low values close or equal to 1 but also has mostly values that are very small.
This should lead to 1-bit flips being effective as the small values can remove the small differences.
Because there are so many small values moving only one bit might be better than switching two elements.
\begin{figure}[h]
      \caption{Distribution of a random geometric input}
      \centering
      \includegraphics[width=0.7\textwidth]{figures/images/numberGenerator/geometricDistributionForp0_001.png}\label{fig:geoDistExample}
\end{figure}
\subsection{RLS Comparison}
\input{tables/geometric/rls_compare.tex}

For these inputs the variants of the RLS perform differently to the binomial input.
The only similarity is the RLS being the only algorithm that did not find an optimal solution for every input.
If the RLS did find an optimal solution in those 21 cases it instead might be the best RLS variant.
The other algorithms are ranked by their probability of flipping only one bit.
This means at first the three \RLSR[s] variants from 2 to 3 to 4 and then the same for the \RLSN[b] variants.
So it does seem like moving mostly one element at once is better for the geometric input in comparison to two elements for the binomial distribution.
In the 21 cases where the RLS did not find an optimal solution it was most likely stuck in a local optimum where no small value was left.

\subsection{(1+1) EA Comparison}
\input{tables/geometric/ea_compare.tex}

The results for the (1+1) EA are similar to the results of the RLS. From mutation rate $2/n$ on the runtime increases with rising mutation rate.
The only part that does not fit into the theory of 1 bit flips being superior is the mutation rate $2/n$ performing better than the standard $1/n$.
All variants reach an optimal solution within the given limit for the number of iterations.
\subsection{pmut Comparison}
\input{tables/geometric/pmut_compare.tex}

The results for the $pmut_\beta$ operator are even more clear than for the (1+1) EA.\
With decreasing values for $\beta$ the amount of flipped bits per step increases.
The performance decreases as well with decreasing values for $\beta$ which fits into the theory of one bit flips being better for geometric distributed inputs.
The number of repetitions of the algorithm might simply be too small to make the small difference in the performance between the two values visible.
The difference in the performance for the $pmut_\beta$ operator is not as drastic as for the (1+1) EA.
Only $\beta=1.5$ and $\beta=1.25$ perform significantly worse the next best value.

\subsection{Comparison of the best variants}
% \input{tables/geometric/best.tex}
% 
The setup for the evaluation of lower values for $n$ is mostly the same except for having a fixed time limit of 100,000 instead of using 50,000 as the limit.
The first try was executed with 50,000 but there the algorithms performed too bad for $n=20$.
Therefore in the second attempt the step limit was increased to 100,000.
The first table lists the number of runs where the different algorithms did not find the optimal solution within the time limit.

\input{tables/geometric/multipleN_fails.tex}

For small inputs the geometric distributed input seems to have inputs without a perfect partition because there were many iterations where neither of the algorithms found an optimal solution within the time limit.
It is still likely to have a perfect partition even for the small values in comparison to other distributions which follow afterwards.
Many algorithms especially the variants of the RLS seem to be likely to get stuck in a local optimum.
The (1+1) EA finds an optimum in most of the runs, so the geometric distributed inputs also seem to be likely to have a perfect partition for small values.
They definitely are harder to solve for smaller input sizes than the binomial inputs, but they still have a perfect partition most times.
The next table visualises the average number of iterations the algorithms needed for finding an optimal solution if the algorithm managed to do so.

\input{tables/geometric/multipleN_avg.tex}

The variants of the (1+1) EA and of the $pmut$ algorithm seem to take about 20,000 iterations for $n=20$ if they manage to find the optimal solution.
They also perform better and better the bigger the input gets.
This is probability caused by the many additional small values that can be used for smaller adjustments to the fitness.
Also a really high value does not have as much of an effect, because there are possibly other larger values which cancel each other out, if they are in different bins.
The standard (1+1) EA does not only find a perfect partition less often, it also needs more iterations on average if it does.
So the (1+1) EA with $p_m=2/n$ performs indeed better for every input size.
The last table again lists the total average number of steps.

\input{tables/geometric/multipleN_totalAvg.tex}
The RLS is only an option if the input is large enough ($n \ge 10,000$). For smaller input sizes especially for $n \le 100$ choosing the (1+1) EA with mutation rate $2/n$ seems like the best choice. For larger values this (1+1) EA does not find an optimal solution the fastest but is still fast enough to be a viable option. Another rather save option is $pmut_{3.25}$. This algorithm performs worse for $n \le 100$ but is still good in comparison to the other algorithms. For $n \ge 1000$ $pmut_{3.25}$ starts to outperform the best version of the (1+1) EA and almost all other researched algorithms.
